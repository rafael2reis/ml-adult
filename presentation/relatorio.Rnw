\documentclass{article}
\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[backend=bibtex, sorting=none]{biblatex}
\usepackage{hyperref}
\usepackage{mathtools}
\bibliography{references}

<<setup, include=FALSE, cache=FALSE, echo=FALSE>>=
Sys.setenv(TEXINPUTS=getwd(),
           BIBINPUTS=getwd(),
           BSTINPUTS=getwd())
@

\sloppy

\title{Um Classificador de Renda\\ para o Adult Data Set}
\author{Luis Felipe Müller \\ \texttt{lhenriques@inf.puc-rio.br}
  \and Rafael dos Reis Silva \\ \texttt{rrsilva@inf.puc-rio.br}}

\begin{document}
\SweaveOpts{concordance=TRUE}

\maketitle

\begin{abstract}
Este trabalho tem como objetivo construir um classificador binário de renda para o Adult Data Set, cuja acurácia seja melhor do que melhor já encontrada na literatura (85.9\%). Foram gerados alguns modelos de aprendizado de máquina a partir de diferentes algoritmos. Aqueles baseados em árvores de decisão obtiveram os melhores resultados, atingindo uma acurácia de mais de 99\%.
\end{abstract}

\section{Introdução}

Classificadores binários são modelos clássicos de aprendizado de máquina. Um exemplo é a tarefa de classficar uma mensagem como \emph{spam} ou não-\emph{spam}. Esses problemas pertecem à categoria de aprendizado \emph{supervisionado}, que consiste em predizer, ou estimar, uma \emph{saída} baseada e uma ou mais \emph{entradas} \cite{James:2014:ISL:2517747}.

\section{Base de Dados}

O Adult Data Set \cite{adultwebsite} é uma base de dados do UCI Machine Learning Repository \cite{Lichman:2013}, gerada a partir de dados do censo americano de 1994. O \emph{data set} possui 15 atributos, sendo 9 categóricos e 6 numéricos, como idade, educação, estado civil, dentre outros. São disponibilizados 2 arquivos, um para o conjunto de treino (com 32.561 registros) e o outro de teste (com 16.281).

O \emph{data set} foi citado pela primeira vez em \cite{kohavi-nbtree} e, desde então, é utilizado, principalmente, para medir a qualidade de novos métodos de aprendizado de máquina \cite{pavlov2000scaling} \cite{cano2003using} \cite{hardt2012simple}.

\section{Tarefa}

A tarefa associada a esta base é uma classficação binária: predizer se uma pessoa possui renda maior do que \$50K ao ano. O atributo \emph{over50K} contém essa informação: ele possui valor ``>50K''  se a pessoa apresenta renda maior do que \$50K ou ``<=50K''  caso contrário.

No conjunto de treino, 76.07\% dos registros estão classificados como ``>50K'', que será utilizado como \emph{baseline} estatístico. Junto com as melhores acurácias encontradas na literatura para esta tarefa \cite{kohavi1996scaling}, temos a seguinte tabela:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Algo & Acurácia (\%) \\
\hline
NBTree & 85.90 \\
\hline
Decision Tree & 84.46 \\
\hline
Naive Bayes & 83.88 \\
\hline
\emph{Baseline} estatístico & 76.07 \\
\hline
\end{tabular}
\caption{Acurácia para a tarefa do Adult Data Set}
\label{table:acuracia}
\end{table}

O objetivo deste trabalho é atingir uma acurácia melhor do que as apresentadas.

\section{Preparação dos Dados}

Os arquivos com os dados foram tratados ...

\section{Classificadores}

Foram utilizados cinco classificadores: Naive Bayes, NBTree, RandomTree, RandomForest e RandomCommittee. Os dois primeiros foram construídos apenas com fins ilustrativos, a fim de atingir os resultados já apresentados na literatura. Os três últimos tiveram como objetivo melhorar a melhor acurácia registrada até então para o \emph{Adult Data Set}. A seguir, uma descrição de cada algoritmo.

\subsection{Naive Bayes}

O classificador Naive Bayes é baseado na suposição de que os valores dos atributos de entrada são condicionalmente independentes do valor da saída \cite{michalski2013machine}. A classificação é dada pela fórmula:

$$v_{NB} = \text{argmax}_{v_j \in V} P(v_j) \prod_i P(a_i|v_j)$$

Onde os diversos valores de $P(v_j)$ e $P(a_i|v_j)$ são estimados utilizando-se o conjunto de treino.

\subsection{NBTree}

NBTree foi proposto em \cite{kohavi1996scaling} e é um híbrido entre as árvores de decisão e o Naive Bayes: os nós da árvore de decisão contêm partições univariadas como os das árvores de decisão ordinárias, mas as folhas contêm classificadores Naive Bayes.

Especialmente, em bases de dados maiores, os resultados do NBTree são melhores do que os da árvores de decisão e do Naive Bayes \cite{kohavi1996scaling}.

\subsection{Random Tree}

Random Tree é uma variante da árvore de decisão, na qual o particionamento de cada nó é escolhido aleatoriamente de um total de $K$ melhores particionamentos \cite{dietterich2000experimental}.

\subsection{Random Forest}

\cite{breiman2001random}

\subsection{Random Committee}

A ideia do Ramdom Committee é bastante simples: ele constrói um comitê de classificadores base e calcula a média das suas predições. Cada classificador é baseado no mesmo conjunto de dados, mas utiliza uma semente diferente para a geração de números aleatórios. Isso só faz sentido quando o classificador base é de natureza aleatória \cite{witten2005data}. Neste trabalho, o classificador base utilizado foi o Random Tree.

\section{Resultados}

Para gerar os resultados, foi utilizado o framework para aprendizado de máquina Weka. Os modelos foram construídos e validados utilizando-se, respectivamente, o conjunto de treino e o de teste fornecidos em 

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
TP Rate & FP Rate & Precision & Recall & F-Measure & ROC Area & Class \\
\hline
0.935 & 0.489 & 0.859 & 0.935 & 0.895 & 0.889 & <=50K \\
\hline
0.511 & 0.065 & 0.712 & 0.511 & 0.595 & 0.889 & >50K \\
\hline
\end{tabular}
\caption{Resultados por Classe: Naive Bayes}
\label{table:1}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
a & b & $\leftarrow$ classified as \\
\hline
11579 & 805 & a = <=50K \\
\hline
1905 & 1992 & b = >50K \\
\hline
\end{tabular}
\caption{Matrix de Confusão Naive Bayes}
\label{table:2}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
TP Rate & FP Rate & Precision & Recall & F-Measure & ROC Area & Class \\
\hline
0.933 & 0.342 & 0.897 & 0.933 & 0.914 & 0.919 & <=50K \\
\hline
0.658 & 0.067 & 0.756 & 0.658 & 0.703 & 0.919 & >50K \\
\hline
\end{tabular}
\caption{Resultados por Classe: NBTree}
\label{table:3}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
 a & b & $\leftarrow$ classified as \\
 \hline
11555 & 829 & a = <=50K \\
\hline
1334 & 2563 & b = >50K \\
 \hline
\end{tabular}
\caption{Matrix de Confusão NBTree}
\label{table:4}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
TP Rate & FP Rate & Precision & Recall & F-Measure & ROC Area & Class \\
\hline
0.999 & 0.007 & 0.998 & 0.999 & 0.999 & 1 & <=50K \\
\hline
0.993 & 0.001 & 0.998 & 0.993 & 0.995 & 1 & >50K \\
\hline
\end{tabular}
\caption{Resultados por Classe: Random Tree}
\label{table:5}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
 a & b & $\leftarrow$ classified as \\
 \hline
 12376 & 8 & a = <=50K \\
 \hline
 28 & 3869 & b = >50K \\
 \hline
\end{tabular}
\caption{Matrix de Confusão Random Tree}
\label{table:6}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
TP Rate & FP Rate & Precision & Recall & F-Measure & ROC Area & Class \\
\hline
1 & 0.007 & 0.998 & 1 & 0.999 & 1 & <=50K \\
\hline
0.993 & 0 & 0.999 & 0.993 & 0.996 & 1 & >50K \\
\hline
\end{tabular}
\caption{Resultados por Classe: Random Forest}
\label{table:7}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
 a & b & $\leftarrow$ classified as \\
 \hline
12382 & 2 & a = <=50K \\
\hline
28 & 3869 & b = >50K \\
 \hline
\end{tabular}
\caption{Matrix de Confusão Random Forest}
\label{table:8}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
TP Rate & FP Rate & Precision & Recall & F-Measure & ROC Area & Class \\
\hline
1 & 0.002 & 0.999 & 1 & 1 & 1 & <=50K \\
\hline
0.998 & 0 & 1 & 0.998 & 0.999 & 1 & >50K \\
\hline
\end{tabular}
\caption{Resultados por Classe: Random Committee}
\label{table:9}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
a & b & $\leftarrow$ classified as \\
\hline
12384 & 0 & a = <=50K \\
\hline
9 & 3888 & b = >50K \\
\hline
\end{tabular}
\caption{Matrix de Confusão Random Committee}
\label{table:10}
\end{table}

\section{Conclusão}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
 Algo & Acurácia(\%) \\
\hline
Naive Bayes & 83.3548 \\
\hline
NBTree & 86.7146 \\
\hline
Random Tree & 99.7789 \\
\hline
Random Forest & 99.8157 \\
\hline
Random Committee & 99.9447 \\
 \hline
\end{tabular}
\caption{Matrix de Confusão Random Committee}
\label{table:resultados}
\end{table}

\printbibliography

\end{document}